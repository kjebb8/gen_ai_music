num_files = 50

seq_length = 25
vocab_size = 128
take_last = False

log_transform(seq):
    seq[:, 1] = (np.log(seq[:, 1] + 0.001) + 4) / 4
    seq[:, 2] = (np.log(seq[:, 2]) + 1.5) / 2

training_ratio = 0.8
batch_size = 64

out_dims = {
    'pitch': 128,
    'step': 1,
    'duration': 1
}
hidden_state_dim = 512
layer_dim = 1
dropout = 0

loss_weights = {
    'pitch': 0.1,
    'step': 2.0,
    'duration': 1.0
}

loss_fns = {
    'pitch': nn.CrossEntropyLoss(),
    'step': nn.MSELoss(),
    'duration': nn.MSELoss()
}

learning_rate = 1e-4

n_epochs = 100
print_progess = 2000

patience = 70